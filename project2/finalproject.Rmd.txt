---
title: "finalpro"
author: "Lina"
date: "December 6, 2018"
output: word_document
---
##PartI: data observation and transformation
###randomly select 2500 observations from the full data set

```{r}
library(dplyr)
hospital <- read.csv("C:/Users/gln_d/Desktop/rutgers class/interperation of data/final project/hospital/hospital.csv")
df <- hospital %>% group_by(STATE) %>% summarise(number = n())
hospitals <- sample(nrow(hospital),2500, replace = FALSE)
hospitalp <- hospital[hospitals,]
new_df <- hospitalp %>% group_by(STATE) %>% summarise(number = n())
df_compare <- cbind(df, new_df)
```

###define response variable SALES and certain predictors

```{r}
hospitalf <- hospitalp %>% mutate(SALES = SALES12 + SALESY)
hospitalf <- hospitalf %>% mutate(gr = TH*4 + TRAUMA*2 + REHAB)
```

###data transformation

```{r}
hospitalfd <- hospitalf[,c("BEDS", "RBEDS", "OUTV", "ADM", "SIR", "TH", "TRAUMA", "REHAB", "HIP95", "KNEE95", "HIP96", "KNEE96", "FEMUR96","SALES")]
par(mfrow=c(3, 2))
mapply(hist,breaks = 35, as.data.frame(hospitalfd),main=colnames(hospitalfd),xlab="x")
```

```{r}
hospit <- hospitalf[,c("BEDS","RBEDS", "HIP95", "KNEE95", "HIP96", "KNEE96", "FEMUR96", "OUTV", "ADM", "SIR", "SALES","gr","TH","TRAUMA","REHAB")]
hospit1 <- hospit %>% mutate(bedsl=log(1+0.1*BEDS),rbedl=log(1+0.02*RBEDS),hip95l=log(1+0.1*HIP95),knee95l=log(1+0.1*KNEE95),knee96l=log(1+0.1*KNEE96),hip96l=log(1+0.1*HIP96),femur96l=log(1+0.1*FEMUR96))
hospit2 <- hospit1 %>% mutate(outvl=log(1+0.0001*OUTV), adml=log(1+0.001*ADM), sirl=log(1+0.001*SIR),salesl=log(1+SALES))
```


```{r}
hospitf <- hospit2[,-c(1:11)]
par(mfrow=c(3, 2))
mapply(hist,breaks = 35, as.data.frame(hospitf),main=colnames(hospitf),xlab="x")
```

```{r}
write.csv(hospitalp,'hospitalp.csv')
write.csv(hospitf,'hospitf.csv')
```

##PartII: PCA: dimension reduction

```{r}
library(tidyverse)  # data manipulation and visualization
library(gridExtra)
library(stats)
hospitfx <- hospitf[,-c(1,15)] #set up X dataset
```

```{r}
hospitfx1 <- hospitfx[,c(6:10)]
hospitfx2 <- hospitfx[,-c(6:10)]
```

```{r}
pca_result1 <- prcomp(hospitfx1, scale = TRUE)
names(pca_result1)
# means
pca_result1$center
# standard deviations
pca_result1$scale
#The rotation matrix provides the principal component loadings; each column of pca_result$rotation contains the corresponding principal component loading vector
pca_result1$rotation
```

```{r}
pca_result2 <- prcomp(hospitfx2, scale = TRUE)
names(pca_result2)
pca_result2$center
pca_result2$scale
pca_result2$rotation
```

###determine the number of PCA

```{r}
#the standard deviation of each principal component
pca_result1$sdev
#The variance explained by each principal component
(VE1 <- pca_result1$sdev^2)
#the proportion of variance explained by each principal component
PVE1 <- VE1 / sum(VE1)
round(PVE1, 2)
```

```{r}
# PVE (aka scree) plot
PVEplot1 <- qplot(c(1:5), PVE1) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("PVE") +
  ggtitle("Scree Plot") +
  ylim(0, 1)
# Cumulative PVE plot
cumPVE1 <- qplot(c(1:5), cumsum(PVE1)) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab(NULL) + 
  ggtitle("Cumulative Scree Plot") +
  ylim(0,1)
grid.arrange(PVEplot1, cumPVE1, ncol = 2)
```

```{r}
#the standard deviation of each principal component
pca_result2$sdev
#The variance explained by each principal component
(VE2 <- pca_result2$sdev^2)
#the proportion of variance explained by each principal component
PVE2 <- VE2 / sum(VE2)
round(PVE2, 2)
```

```{r}
# PVE (aka scree) plot
PVEplot2 <- qplot(c(1:8), PVE2) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("PVE") +
  ggtitle("Scree Plot") +
  ylim(0, 1)
# Cumulative PVE plot
cumPVE2 <- qplot(c(1:8), cumsum(PVE2)) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab(NULL) + 
  ggtitle("Cumulative Scree Plot") +
  ylim(0,1)
grid.arrange(PVEplot2, cumPVE2, ncol = 2)
```

###obtain one PC from PCA analysis I; obtain two PC from the PCA analysis II . Totally 4 PC

###extract the score of PCA 1-4 for each observation and list them as predictors in the new data set for further analysis

```{r}
loadings <- as.data.frame(pca_result2$rotation[,1:3])
axes2 <- predict(pca_result2, hospitfx2)
axes2 <- as.data.frame(axes2)
head(axes2)
dat <- cbind(hospitf, axes2$PC1, axes2$PC2, axes2$PC3)
```

```{r}
loadings <- as.data.frame(pca_result1$rotation[,1])
loadings2 <- as.data.frame(pca_result2$rotation[,1:3])
axes1 <- predict(pca_result1, hospitfx1)
axes1 <- as.data.frame(axes1)
head(axes1)
dat <- cbind(dat, axes1$PC1)
```

##PartIII: cluster analysis

```{r}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
datclu <- dat[,c(16:19)]
```

###(1)K-mean cluster analysis

```{r}
df <- scale(datclu)
head(df)
k2 <- kmeans(df, centers = 2, nstart = 25)
k3 <- kmeans(df, centers = 4, nstart = 25)
k4 <- kmeans(df, centers = 6, nstart = 25)
k5 <- kmeans(df, centers = 8, nstart = 25)
k6 <- kmeans(df, centers = 10, nstart = 25)
k7 <- kmeans(df, centers = 15, nstart = 25)
# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 4")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 6")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 8")
p5 <- fviz_cluster(k6, geom = "point",  data = df) + ggtitle("k = 10")
p6 <- fviz_cluster(k7, geom = "point",  data = df) + ggtitle("k = 15")
library(gridExtra)
grid.arrange(p1, p2, p3, p4, p5,p6,nrow = 2)
```

####elbow and silhouette optimized clusters

```{r}
#Elbow method
set.seed(123)
fviz_nbclust(df, kmeans, method = "wss")
#Average Silhouette Method
fviz_nbclust(df, kmeans, method = "silhouette")
```

```{r}
finale <- kmeans(df, 6, nstart = 25)
dfke <- cbind(finale$cluster, datclu, hospitf)
colnames(dfke)[1] <- "cluster"
dfke$cluster <- as.factor(dfke$cluster)
library(ggplot2)
ggplot(dfke, aes(cluster, salesl)) +
  geom_boxplot()
```
```{r}
colnames(dfke)[2:5] <- c("PC1","PC2","PC3","PC4")
ggplot(dfke, aes(cluster, PC1)) +
  geom_boxplot()
ggplot(dfke, aes(cluster, PC2)) +
  geom_boxplot()
ggplot(dfke, aes(cluster, PC3)) +
  geom_boxplot()
ggplot(dfke, aes(cluster, PC4)) +
  geom_boxplot()
```



```{r}
finals <- kmeans(df, 5, nstart = 25)
dfks <- cbind(finals$cluster, datclu, hospitf)
colnames(dfks)[1] <- "cluster"
dfks$cluster <- as.factor(dfks$cluster)
library(ggplot2)
ggplot(dfks, aes(cluster, salesl)) +
  geom_boxplot()
```

```{r}
colnames(dfks)[2:5] <- c("PC1","PC2","PC3","PC4")
ggplot(dfks, aes(cluster, PC1)) +
  geom_boxplot()
ggplot(dfks, aes(cluster, PC2)) +
  geom_boxplot()
ggplot(dfks, aes(cluster, PC3)) +
  geom_boxplot()
ggplot(dfks, aes(cluster, PC4)) +
  geom_boxplot()
```

####gap statistics clusters
```{r}
#gap
set.seed(125)
gap_stat <- clusGap(df, FUN = kmeans, nstart = 25,
                    K.max = 10, B = 60,iter.max=50)
```

```{r}
fviz_gap_stat(gap_stat)
final <- kmeans(df, 7, nstart = 25)
```

```{r}
dfkg <- cbind(final$cluster, datclu, hospitf)
colnames(dfkg)[1] <- "cluster"
dfkg$cluster <- as.factor(dfkg$cluster)
```

```{r}
library(ggplot2)
ggplot(dfkg, aes(cluster, salesl)) +
  geom_boxplot()
```

```{r}
colnames(dfkg)[2:5] <- c("PC1","PC2","PC3","PC4")
ggplot(dfkg, aes(cluster, PC1)) +
  geom_boxplot()
ggplot(dfkg, aes(cluster, PC2)) +
  geom_boxplot()
ggplot(dfkg, aes(cluster, PC3)) +
  geom_boxplot()
ggplot(dfkg, aes(cluster, PC4)) +
  geom_boxplot()
```

```{r}
print(final$size)
print(finale$size)
print(finals$size)
```

```{r}
dfkep <- dfke[,c(1:5,20)]
write.csv(dfkep,'dfkep.csv')
dfksp <- dfks[,c(1:5,20)]
write.csv(dfks,'dfksp.csv')
dfkgp <- dfkg[,c(1:5,20)]
write.csv(dfkg,'dfkgp.csv')
```

###(2)hierarchical analysis using ward's method

```{r}
#Elbow method
set.seed(124)
fviz_nbclust(df, hcut, method = "wss")
#Average Silhouette Method
fviz_nbclust(df, hcut, method = "silhouette")
```

```{r}
#scale data
df <- scale(df)
# Compute pairewise distance matrices
dist.res <- dist(df, method = "euclidean")
# Hierarchical clustering results
hc <- hclust(dist.res, method = "ward.D")
# Visualization of hclust
plot(hc, labels = FALSE, hang = -1)
# Add rectangle around 3 groups
rect.hclust(hc, k = 6, border = 2:4) 
# Cut into 3 groups
hc.cut <- cutree(hc, k = 6)
```

```{r}
hccute <- cbind(hc.cut, datclu, hospitf)
colnames(hccute)[1] <- "cluster"
hccute$cluster <- as.factor(hccute$cluster)
```

```{r}
library(ggplot2)
ggplot(hccute, aes(cluster, salesl)) +
  geom_boxplot()
```

```{r}
colnames(hccute)[2:5] <- c("PC1","PC2","PC3","PC4")
ggplot(hccute, aes(cluster, PC1)) +
  geom_boxplot()
ggplot(hccute, aes(cluster, PC2)) +
  geom_boxplot()
ggplot(hccute, aes(cluster, PC3)) +
  geom_boxplot()
ggplot(hccute, aes(cluster, PC4)) +
  geom_boxplot()
```

```{r}
plot(hc, labels = FALSE, hang = -1)
# Add rectangle around 3 groups
rect.hclust(hc, k = 5, border = 2:4) 
# Cut into 3 groups
hc.cut2 <- cutree(hc, k = 5)
```

```{r}
hccuts <- cbind(hc.cut2, datclu, hospitf)
colnames(hccuts)[1] <- "cluster"
hccuts$cluster <- as.factor(hccuts$cluster)
```

```{r}
library(ggplot2)
ggplot(hccuts, aes(cluster, salesl)) +
  geom_boxplot()
```

```{r}
colnames(hccuts)[2:5] <- c("PC1","PC2","PC3","PC4")
ggplot(hccuts, aes(cluster, PC1)) +
  geom_boxplot()
ggplot(hccuts, aes(cluster, PC2)) +
  geom_boxplot()
ggplot(hccuts, aes(cluster, PC3)) +
  geom_boxplot()
ggplot(hccuts, aes(cluster, PC4)) +
  geom_boxplot()
```

```{r}
library(dplyr)
 hccute %>% count(cluster)
 hccuts %>% count(cluster)
```

####gap statistics clusters
```{r}
# Compute gap statistic
set.seed(126)
gap_stat2 <- clusGap(df, FUN = hcut, K.max = 30, B = 50)
# Plot gap statistic
fviz_gap_stat(gap_stat2)
```

```{r}
plot(hc, labels = FALSE, hang = -1)
# Add rectangle around 3 groups
rect.hclust(hc, k = 24, border = 2:4) 
# Cut into 3 groups
hc.cutp <- cutree(hc, k = 24)
```
```{r}
hccutp <- cbind(hc.cutp, datclu, hospitf)
colnames(hccutp)[1] <- "cluster"
hccutp$cluster <- as.factor(hccutp$cluster)
```

```{r}
library(ggplot2)
ggplot(hccutp, aes(cluster, salesl)) +
  geom_boxplot()
```

```{r}
colnames(hccutp)[2:5] <- c("PC1","PC2","PC3","PC4")
ggplot(hccutp, aes(cluster, PC1)) +
  geom_boxplot()
ggplot(hccutp, aes(cluster, PC2)) +
  geom_boxplot()
ggplot(hccutp, aes(cluster, PC3)) +
  geom_boxplot()
ggplot(hccutp, aes(cluster, PC4)) +
  geom_boxplot()
```

```{r}
library(dplyr)
 hccutp %>% count(cluster)
```

```{r}
hccutep <- hccute[,c(1:5,20)]
write.csv(hccutep,'hccutep.csv')
hccutpp <- hccutp[,c(1:5,20)]
write.csv(hccutpp,'hccutpp.csv')
hccutsp <- hccuts[,c(1:5,20)]
write.csv(hccutsp,'hccutsp.csv')
```

###(3)robust cluster analysis by pam

```{r}
#Elbow method
set.seed(127)
fviz_nbclust(df, pam, method = "wss")
#Average Silhouette Method
fviz_nbclust(df, pam, method = "silhouette")
```

```{r}
pam.res <- pam(df, 7)
fviz_cluster(pam.res, geom = "point", data = df) + ggtitle("k = 7")
dfpe <- cbind(pam.res$clustering, datclu, hospitf)
colnames(dfpe)[1] <- "cluster"
dfpe$cluster <- as.factor(dfpe$cluster)
library(ggplot2)
ggplot(dfpe, aes(cluster, salesl)) +
  geom_boxplot()
```

```{r}
colnames(dfpe)[2:5] <- c("PC1","PC2","PC3","PC4")
ggplot(dfpe, aes(cluster, PC1)) +
  geom_boxplot()
ggplot(dfpe, aes(cluster, PC2)) +
  geom_boxplot()
ggplot(dfpe, aes(cluster, PC3)) +
  geom_boxplot()
ggplot(dfpe, aes(cluster, PC4)) +
  geom_boxplot()
```

```{r}
library(dplyr)
  dfpe %>% count(cluster)
```

####gap statistics clusters
```{r}
# Compute gap statistic
set.seed(128)
gap_stat3 <- clusGap(df, FUN = pam, K.max = 20, B = 50)
# Plot gap statistic
fviz_gap_stat(gap_stat3)
```

```{r}
pam.res2 <- pam(df, 13)
fviz_cluster(pam.res2, geom = "point", data = df) + ggtitle("k = 13")
dfpg <- cbind(pam.res2$clustering, datclu, hospitf)
colnames(dfpg)[1] <- "cluster"
dfpg$cluster <- as.factor(dfpg$cluster)
ggplot(dfpg, aes(cluster, salesl)) +
  geom_boxplot()
```

```{r}
colnames(dfpg)[2:5] <- c("PC1","PC2","PC3","PC4")
ggplot(dfpg, aes(cluster, PC1)) +
  geom_boxplot()
ggplot(dfpg, aes(cluster, PC2)) +
  geom_boxplot()
ggplot(dfpg, aes(cluster, PC3)) +
  geom_boxplot()
ggplot(dfpg, aes(cluster, PC4)) +
  geom_boxplot()
```
```{r}
library(dplyr)
  dfpg %>% count(cluster)
```

```{r}
dfpep <- dfpe[,c(1:5,20)]
write.csv(dfpep,'dfpep.csv')
dfpgp<- dfpg[,c(1:5,20)]
write.csv(dfpgp,'dfpgp.csv')
```


###Part VI:regression tree analysis using CART

```{r}
library(rpart)
library(caret)
library(rpart.plot)
dftr <- cbind(dat$salesl,df)
colnames(dftr) <- c("salesl","pc1","pc2","pc3","pc4")
#use original data to do decision tree
```

```{r}
hospital = read.csv("hospital.csv")
library(rpart)
hosp = hospital[1:1000,-c(1:4,10)]
hosp$TH = factor(hosp$TH)     
hosp$TRAUMA = factor(hosp$TRAUMA)     
hosp$REHAB = factor(hosp$REHAB)  
u=rpart(log(1+SALES12)~.,data=hosp,control=rpart.control(cp=.01))
plot(u);  text(u)
u=rpart(log(1+SALES12)~.,data=hosp,control=rpart.control(cp=.001))
plot(u,uniform=T) ; text(u)
```

```{r}
library(rpart)
htree = rpart(y~., data=hosp)
par(cex=0.5); plot(htree) ; text(htree)
plot(hosp[,12],y)
u=rpart(y~.,data=hosp,control=rpart.control(cp=.005))
u
plot(u)
u=rpart(y~.,data=hosp,control=rpart.control(cp=.0001))
plot(u)
u
hosp[1,]
u=rpart(y~.,data=hosp[,c(-12,-7)],control=rpart.control(cp=.0001))
u
u=rpart(y~.,data=hosp[,c(-12,-7)],control=rpart.control(cp=.001))
plot(u)
```


```{r}
pred <- predict(object = grade_model,   # model object 
                newdata = grade_test)  # test dataset
library(metrics)
rmse(actual = grade_test$final_grade, predicted = pred)

#tune the tree
# Plot the "CP Table"
plotcp(grade_model)
# Print the "CP Table"
print(grade_model$cptable)
# Retrieve optimal cp value based on cross-validated error
opt_index <- which.min(grade_model$cptable[, "xerror"])
cp_opt <- grade_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
grade_model_opt <- prune(tree = grade_model, 
                         cp = cp_opt)
# Plot the optimized model
rpart.plot(x = grade_model_opt, yesno = 2, type = 0, extra = 0)
```

